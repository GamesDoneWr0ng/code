diff --git a/python/pong/ai.py b/python/pong/ai.py
index b07e3d1..05e1675 100644
--- a/python/pong/ai.py
+++ b/python/pong/ai.py
@@ -32,7 +32,7 @@ def parse_args():
                         help='if toggled, `torch.backends.cudnn.deterministic=False`')
     parser.add_argument('--mps', type=lambda x:bool(strtobool(x)), default=True, nargs='?', const=True,
                         help='if toggled, mps will not be enabled by default')
-    parser.add_argument('--track', type=lambda x:bool(strtobool(x)), default=False, nargs='?', const=True,
+    parser.add_argument('--track', type=lambda x:bool(strtobool(x)), default=True, nargs='?', const=True,
                         help='if toggled, this experiment will be tracked with Weights and Biases')
     parser.add_argument('--wandb-project-name', type=str, default="cleanRL",
                         help="the wandb's project name")
@@ -40,7 +40,7 @@ def parse_args():
                         help="the entity (team) of wandb's project")
     parser.add_argument('--capture-video', type=lambda x:bool(strtobool(x)), default=True, nargs='?', const=True,
                         help='weather to capture videos of the agent performances (check out `videos` folder)')
-    parser.add_argument('--save_name', type=str, default='main',
+    parser.add_argument('--save_name', type=str, default='new',
                         help='the name of the file to save the model')
     
     # Algorithm specific arguments
@@ -77,34 +77,6 @@ def parse_args():
     args.minibatch_size = int(args.batch_size // args.num_minibatches)
     return args
 
-args = parse_args()
-print(args)
-run_name = f"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
-
-if args.track:
-    import wandb
-    wandb.init(
-        project=args.wandb_project_name,
-        entity=args.wandb_entity,
-        sync_tensorboard=False,
-        config=vars(args),
-        name=run_name,
-        monitor_gym=True,
-        save_code=True
-    )
-writer = SummaryWriter(f"python/pong/runs/{run_name}")
-writer.add_text(
-    "hyperparameters",
-    "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-)
-
-random.seed(args.seed)
-np.random.seed(args.seed)
-torch.manual_seed(args.seed)
-torch.backends.cudnn.deterministic = args.torch_deterministic
-
-device = torch.device("mps" if torch.backends.mps.is_available() and args.mps else "cpu")
-
 def make_env(gym_id, seed, idx, capture_video, run_name):
     def thunk():
         env = gym.make(gym_id)
@@ -159,161 +131,190 @@ class Agent(nn.Module):
         if os.path.isfile(filename):
             self.load_state_dict(torch.load(filename))
 
-# env setup
-envs = gym.vector.SyncVectorEnv(
-    [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) 
-     for i in range(args.num_envs)])
-assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
-
-agent = Agent(envs).to(device)
-agent.load("python/pong/policies/" + args.save_name + ".pt") # load the trained policy
-optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
-
-# Algo logic: Storage setup
-obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
-actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
-logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
-rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
-dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
-values = torch.zeros((args.num_steps, args.num_envs)).to(device)
-
-# TRY NOT TO MODIFY: start the game
-with open("python/pong/policies/timesteps.pkl", 'rb') as f: # load timestep for current policy
-    timesteps = pickle.load(f)
-    if args.save_name in timesteps:    
-        global_step = timesteps[args.save_name]
-    else:
-        global_step = 0
-        timesteps[args.save_name] = 0
-#global_step = 0
-start_time = time.time()
-next_obs = torch.Tensor(envs.reset()[0]).to(device)
-next_done = torch.zeros(args.num_envs).to(device)
-num_updates = (args.total_timesteps - global_step) // args.num_steps
-
-for update in range(1, num_updates + 1):
-    agent.save("python/pong/policies/" + args.save_name + ".pt")
-    with open("python/pong/policies/timesteps.pkl", 'wb') as f:
-        timesteps[args.save_name] = global_step
-        pickle.dump(timesteps, f)
-    # Anneling the rate if instructed to do so.
-    if args.anneal_lr:
-        frac = 1.0 - (update - 1.0) / num_updates
-        lrnow = frac * args.learning_rate
-        optimizer.param_groups[0]['lr'] = lrnow
-
-    for step in range(0, args.num_steps):
-        global_step += 1 * args.num_envs
-        obs[step] = next_obs
-        dones[step] = next_done
-
-        # Algo logic: action logic
-        with torch.no_grad():
-            action, logprob, _, value = agent.get_action_and_value(next_obs)
-            values[step] = value.flatten()
-        actions[step] = action
-        logprobs[step] = logprob
-
-        # TRY NOT TO MODIFY: execute the game and log data.
-        next_obs, reward, done, idk, info = envs.step(action[0].cpu().numpy()) # e
-        rewards[step] = torch.Tensor(reward).to(device).view(-1)
-        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)
-
-        for index, i in enumerate(done):
-            if i:
-                print(f'global timestep: {global_step}, episodic_reward: {info["final_info"][index]["episode"]["r"]}')
-                writer.add_scalar("charts/episodic_return", info["final_info"][index]["episode"]["r"], global_step)
-                writer.add_scalar("charts/episodic_length", info["final_info"][index]["episode"]["l"], global_step)
-
-    # bootstrap reward if not done.
-    with torch.no_grad():
-        next_value = agent.get_value(next_obs).reshape(1, -1)
-        if args.gae:
-            advantages = torch.zeros_like(rewards).to(device)
-            lastgaelam = 0
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    nextvalues = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t+1]
-                    nextvalues = values[t+1]
-                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
-                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
-            returns = advantages + values
+if __name__ == "__main__":
+    args = parse_args()
+    print(args)
+    run_name = f"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    
+    if args.track:
+        import wandb
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=False,
+            config=vars(args),
+            name=run_name,
+            monitor_gym=True,
+            save_code=True
+        )
+    writer = SummaryWriter(f"/Users/askborgen/Desktop/code/python/pong/runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+    
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+    
+    device = torch.device("mps" if torch.backends.mps.is_available() and args.mps else "cpu")
+    
+    # env setup
+    envs = gym.vector.SyncVectorEnv(
+        [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) 
+         for i in range(args.num_envs)])
+    assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"
+    
+    agent = Agent(envs).to(device)
+    agent.load("python/pong/policies/" + args.save_name + ".pt") # load the trained policy
+    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
+    
+    # Algo logic: Storage setup
+    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
+    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
+    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    
+    # TRY NOT TO MODIFY: start the game
+    with open("python/pong/policies/timesteps.pkl", 'rb') as f: # load timestep for current policy
+        timesteps = pickle.load(f)
+        if args.save_name in timesteps:    
+            global_step = timesteps[args.save_name]
         else:
-            returns = torch.zeros_like(rewards).to(device)
-            for t in reversed(range(args.num_steps)):
-                if t == args.num_steps - 1:
-                    nextnonterminal = 1.0 - next_done
-                    next_return = next_value
-                else:
-                    nextnonterminal = 1.0 - dones[t+1]
-                    next_return = returns[t+1]
-                returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return
-            returns = advantages - values
-
-    # flatten the batch
-    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
-    b_logprobs = logprobs.reshape(-1)
-    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
-    b_advantages = advantages.reshape(-1)
-    b_returns = returns.reshape(-1)
-    b_values = values.reshape(-1)
-
-    # optimizing the policy and the value network
-    b_inds = np.arange(args.batch_size)
-    for epoch in range(args.update_epochs):
-        np.random.shuffle(b_inds)
-        for start in range(0, args.batch_size, args.minibatch_size):
-            end = start + args.minibatch_size
-            mb_inds = b_inds[start:end]
-                
-            _, newlogprob, entropy, newvalue = agent.get_action_and_value(
-                b_obs[mb_inds], b_actions.long()[mb_inds]
-            )
-            logratio = newlogprob - b_logprobs[mb_inds]
-            ratio = logratio.exp()
-                
-            mb_advantages = b_advantages[mb_inds]
-            if args.norm_adv:
-                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
-
-            # Policy loss
-            pg_loss1 = -mb_advantages * ratio
-            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
-            pg_loss = torch.max(pg_loss1, pg_loss2).mean()
-
-            # Value loss
-            newvalue = newvalue.view(-1)
-            if args.clip_vloss:
-                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
-                v_clipped = b_values[mb_inds] + torch.clamp(
-                    newvalue - b_values[mb_inds],
-                    -args.clip_coef,
-                    args.clip_coef
-                )
-                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
-                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
-                v_loss = 0.5 * v_loss_max.mean()
+            global_step = 0
+            timesteps[args.save_name] = 0
+    #global_step = 0
+    start_time = time.time()
+    next_obs = torch.Tensor(envs.reset()[0]).to(device)
+    next_done = torch.zeros(args.num_envs).to(device)
+    num_updates = (args.total_timesteps - global_step) // args.num_steps
+    
+    for update in range(1, num_updates + 1):
+        agent.save("python/pong/policies/" + args.save_name + ".pt")
+        with open("python/pong/policies/timesteps.pkl", 'wb') as f:
+            timesteps[args.save_name] = global_step
+            pickle.dump(timesteps, f)
+        # Anneling the rate if instructed to do so.
+        if args.anneal_lr:
+            frac = 1.0 - (update - 1.0) / num_updates
+            lrnow = frac * args.learning_rate
+            optimizer.param_groups[0]['lr'] = lrnow
+    
+        for step in range(0, args.num_steps):
+            global_step += 1 * args.num_envs
+            obs[step] = next_obs
+            dones[step] = next_done
+    
+            # Algo logic: action logic
+            with torch.no_grad():
+                action, logprob, _, value = agent.get_action_and_value(next_obs)
+                values[step] = value.flatten()
+            actions[step] = action
+            logprobs[step] = logprob
+    
+            # TRY NOT TO MODIFY: execute the game and log data.
+            next_obs, reward, done, idk, info = envs.step(action[0].cpu().numpy()) # e
+            rewards[step] = torch.Tensor(reward).to(device).view(-1)
+            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)
+    
+            for index, i in enumerate(done):
+                if i:
+                    print(f'global timestep: {global_step}, episodic_reward: {info["final_info"][index]["episode"]["r"]}')
+                    writer.add_scalar("charts/episodic_return", info["final_info"][index]["episode"]["r"], global_step)
+                    writer.add_scalar("charts/episodic_length", info["final_info"][index]["episode"]["l"], global_step)
+    
+        # bootstrap reward if not done.
+        with torch.no_grad():
+            next_value = agent.get_value(next_obs).reshape(1, -1)
+            if args.gae:
+                advantages = torch.zeros_like(rewards).to(device)
+                lastgaelam = 0
+                for t in reversed(range(args.num_steps)):
+                    if t == args.num_steps - 1:
+                        nextnonterminal = 1.0 - next_done
+                        nextvalues = next_value
+                    else:
+                        nextnonterminal = 1.0 - dones[t+1]
+                        nextvalues = values[t+1]
+                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
+                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
+                returns = advantages + values
             else:
-                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
-
-            entropy_loss = entropy.mean()
-            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
-
-            optimizer.zero_grad()
-            loss.backward()
-            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
-            optimizer.step()
-
-    # TRY NOT TO MODIFY: record rewards for plotting purposes
-        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
-        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
-        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
-        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
-        print("SPS:", int(global_step / (time.time() - start_time)))
-        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
-
-envs.close()
-writer.close()
\ No newline at end of file
+                returns = torch.zeros_like(rewards).to(device)
+                for t in reversed(range(args.num_steps)):
+                    if t == args.num_steps - 1:
+                        nextnonterminal = 1.0 - next_done
+                        next_return = next_value
+                    else:
+                        nextnonterminal = 1.0 - dones[t+1]
+                        next_return = returns[t+1]
+                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return
+                returns = advantages - values
+    
+        # flatten the batch
+        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
+        b_logprobs = logprobs.reshape(-1)
+        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
+        b_advantages = advantages.reshape(-1)
+        b_returns = returns.reshape(-1)
+        b_values = values.reshape(-1)
+    
+        # optimizing the policy and the value network
+        b_inds = np.arange(args.batch_size)
+        for epoch in range(args.update_epochs):
+            np.random.shuffle(b_inds)
+            for start in range(0, args.batch_size, args.minibatch_size):
+                end = start + args.minibatch_size
+                mb_inds = b_inds[start:end]
+                    
+                _, newlogprob, entropy, newvalue = agent.get_action_and_value(
+                    b_obs[mb_inds], b_actions.long()[mb_inds]
+                )
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+                    
+                mb_advantages = b_advantages[mb_inds]
+                if args.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
+    
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
+                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
+    
+                # Value loss
+                newvalue = newvalue.view(-1)
+                if args.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + torch.clamp(
+                        newvalue - b_values[mb_inds],
+                        -args.clip_coef,
+                        args.clip_coef
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+    
+                entropy_loss = entropy.mean()
+                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
+    
+                optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
+                optimizer.step()
+    
+        # TRY NOT TO MODIFY: record rewards for plotting purposes
+            writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+            writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
+            writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+            writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+            #print("SPS:", int(global_step / (time.time() - start_time)))
+            writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+    
+    envs.close()
+    writer.close()
\ No newline at end of file
diff --git a/python/pong/main.py b/python/pong/main.py
index d1508a2..2ed8790 100644
--- a/python/pong/main.py
+++ b/python/pong/main.py
@@ -2,21 +2,30 @@
 # Handels comunication between classes
 
 import pygame as pg
-import time
+import gymnasium as gym
+from torch import Tensor, tensor, device, no_grad
+from numpy import argmax
+from torch.distributions.categorical import Categorical
 from pong import PongEnv
-#from ai import Ai
+from ai import Agent
 pg.init()
 
 size = width, height = 800, 600
 screen = pg.display.set_mode(size)
 pg.display.set_caption("Pong")
-clock = pg.time.Clock()
-fps = 60
 
 class Main:
     def __init__(self) -> None:
+        def make_env():
+            def thunk():
+                return gym.make("Pong-v0")
+            return thunk
+        
+        env = gym.vector.SyncVectorEnv([make_env()])
+        self.ai = Agent(env)
+        self.ai.load("python/pong/policies/main.pt")
+
         self.pong = PongEnv(size, render_mode="human-vs-bot")
-        #self.ai = Ai()
 
     def inputHandler(self):
         for event in pg.event.get():
@@ -28,14 +37,25 @@ class Main:
         elif keys[pg.K_s] or keys[pg.K_DOWN]:
             return 4
         return 0
+    
+    def getAi(self, obs):
+        with no_grad(): # ai
+            logits = self.ai.actor(obs)
+            probs = Categorical(logits=logits)
+            #probs = torch.nn.functional.softmax(logits, dim=-1)
+            #action = probs.cpu().sample()
+            action = tensor(argmax(probs.probs.cpu().numpy(), keepdims=True).T, device=device("mps"))
+            return action.cpu().numpy()[0]
+
 
 main = Main()
 
+obs = Tensor(main.pong.reset()[0])
 main.running = True
 while main.running:
-    clock.tick(fps)
-
     inputs = main.inputHandler()
-    #ai = main.ai.update(main.pong.getState())
-    main.pong.step(0, inputs)#, ai)
+    #opponent = main.getAi(obs)
+    opponent = int(main.pong.ballPos[1] > main.pong.aiPaddle)
+
+    obs = Tensor(main.pong.step(opponent, inputs)[0])
     main.pong.render()
\ No newline at end of file
diff --git a/python/pong/policies/main.pt b/python/pong/policies/main.pt
index fec16b9..dde7f66 100644
Binary files a/python/pong/policies/main.pt and b/python/pong/policies/main.pt differ
diff --git a/python/pong/policies/timesteps.pkl b/python/pong/policies/timesteps.pkl
index b452097..9c6e43c 100644
Binary files a/python/pong/policies/timesteps.pkl and b/python/pong/policies/timesteps.pkl differ
diff --git a/python/pong/pong.py b/python/pong/pong.py
index 18e2002..aab2ff7 100644
--- a/python/pong/pong.py
+++ b/python/pong/pong.py
@@ -6,7 +6,7 @@ from gymnasium import spaces
 class PongEnv(gym.Env):
     metadata = {"render_modes": ["human-vs-bot", "training", "rgb_array"], "render_fps": 60}
 
-    def __init__(self, size = [800,800], render_mode="rgb_array") -> None:
+    def __init__(self, size = [800,600], render_mode="rgb_array") -> None:
         self.size = size
         self.score = [0,0]
         self.reset()
@@ -162,7 +162,7 @@ class PongEnv(gym.Env):
     def render(self):
         if self.render_mode == "rgb_array" or self.render_mode == "human-vs-bot":
             return self._render_frame()
-        
+
     def _render_frame(self):
         if self.window is None and self.render_mode == "human-vs-bot":
             pg.init()
diff --git a/python/temp.ipynb b/python/temp.ipynb
index cbf20a6..81a7e92 100644
--- a/python/temp.ipynb
+++ b/python/temp.ipynb
@@ -2,26 +2,31 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 2,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "False\n",
+      "False\n",
+      "False\n"
+     ]
+    }
+   ],
    "source": [
-    "from pynput.keyboard import Key, Controller\n",
-    "import time\n",
-    "import threading\n",
+    "A = {1, 2, 3}\n",
+    "B = {1, 2, 3, 4, 5}\n",
     "\n",
-    "# Create a keyboard controller\n",
-    "keyboard = Controller()\n",
+    "# Check if A is a subset of B\n",
+    "print(A.issubset(B))\n",
     "\n",
-    "# Wait for a moment before sending the key press\n",
-    "time.sleep(2)\n",
+    "# Check if B is a superset of A\n",
+    "print(B.issuperset(A))\n",
     "\n",
-    "# Press and hold the \"End\" key\n",
-    "keyboard.press(Key.end)\n",
-    "time.sleep(30)  # Adjust the duration as needed\n",
-    "\n",
-    "# Release the \"End\" key\n",
-    "keyboard.release(Key.end)"
+    "# Check if A and B have no elements in common\n",
+    "print(A.isdisjoint(B))\n"
    ]
   }
  ],
